{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"d4h-uth4lsmG"},"outputs":[],"source":["import numpy as np\n","from os import listdir\n","from os.path import isfile, join\n","# from google.colab import files\n","# from google.colab import drive\n","import cv2\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","from google.colab.patches import cv2_imshow\n","\n","import tensorflow as tf\n","import keras\n","from keras.datasets import mnist\n","import itertools \n","from keras.utils import np_utils\n","from keras.models import Sequential\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator \n","from sklearn.metrics import confusion_matrix \n","from keras import layers\n","from tensorflow.keras.utils import to_categorical\n","#from keras.layers import Dense, Dropout , Input , Flatten , Conv2D , MaxPooling2D, BatchNormalization, Activation\n","from keras.callbacks import EarlyStopping , ModelCheckpoint\n","from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n","from keras.preprocessing.image import img_to_array, load_img\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VksPQ87Z1yrv"},"outputs":[],"source":["#used in blob detector\n","def extract_image_center(img, hcrop=0, wcrop=10):\n","    x = img.shape[0]\n","    y = img.shape[1]\n","    xc = int(hcrop*x/100)\n","    yc = int(wcrop*y/100)\n","    img = img[xc:x-xc, yc:y-yc]\n","    img = cv2.resize(img,(x,y))\n","    return img"]},{"cell_type":"code","source":["#CREATES ROWS FOR EXCEL SHEET.\n","def addSheetRow(sheet, row, ID, startX, startY, endX, endY, occurences, avgRadius):\n","  sheet.write(row,0, str(ID))\n","  sheet.write(row,1, f'{startX:.1f}' + \", \" + f'{startY:.1f}' )\n","  sheet.write(row,2, f'{endX:.1f}' + \", \" + f'{endY:.1f}' )\n","  sheet.write(row,3, '%d' % occurences )\n","  sheet.write(row,4, f'{avgRadius:.3f}' ) #make output limited to 3 decimals"],"metadata":{"id":"iBp1iIdfqGZg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9s_UM1UXkXGa"},"outputs":[],"source":["def generateAnalysisVideo(img_dir, video_dir, total_images):\n","\n","  #get path to image directory\n","  image_directory = [os.path.join(img_dir,f) for f in os.listdir(img_dir)]\n","\n","  fourcc = cv2.VideoWriter_fourcc(*'mp4v') #codec format\n","  video_file = video_dir + \"video.mp4\"\n","  #get path to video directory\n","  #video_directory = [os.path.join(video_file,f) for f in os.listdir(video_file)]\n","\n","\n","  img = []\n","  for img_name in image_directory:\n","    img.append(cv2.imread(img_name))\n","\n","  height,width,layers=img[1].shape\n","  video=cv2.VideoWriter(video_file,fourcc,30,(width,height))\n","\n","  for j in range(0,total_image_count): #create video\n","      video.write(img[j])\n","\n","  cv2.destroyAllWindows()\n","  video.release()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePI2q9f6Aks8"},"outputs":[],"source":["def process_image(image, target_shape):\n","    \"\"\"Given an image, process it and return the array.\"\"\"\n","    # Load the image.\n","    h, w, _ = target_shape\n","    image = load_img(image, target_size=(h, w))\n","\n","    # Turn it into numpy, normalize and return.\n","    img_arr = img_to_array(image)\n","    #x = (img_arr / 255.).astype(np.float32)\n","    x = img_arr\n","\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sDOrFba7Hh3n"},"outputs":[],"source":["#This function will plot images in the form of a grid with 1 row and 10 columns \n","def plotImages(images_arr):\n","    fig, axes = plt.subplots(1, 10, figsize=(100,100))\n","    axes = axes.flatten()\n","    for img, ax in zip( images_arr, axes):\n","        ax.imshow(img)\n","        ax.axis('off')\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iqp0yl6z2Rsj"},"outputs":[],"source":["def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    print(cm)\n","\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, cm[i, j],\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15918,"status":"ok","timestamp":1649610729671,"user":{"displayName":"Travis Howard","userId":"04316066016894535489"},"user_tz":420},"id":"RohzkBpwl5ai","outputId":"60073691-480c-4c0f-9b35-02fc35ca0d17"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"FCPSoQT9r7cu"},"source":["# Making predictions with best CNN Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BGQBQjgfl59i"},"outputs":[],"source":["from keras.models import load_model\n","\n","model = load_model('/content/drive/MyDrive/cse485/CNN_model_best.h5') #model_EcoFilter_Best.h5\n","#model.compile()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jUXHzvNnQ6jI"},"outputs":[],"source":["test_image = process_image(\"/content/drive/MyDrive/cse485/data/test02/dirty/20210226_143748.jpg\",( 150,150, 3)) #height,width, color channels\n","#test_image = cv2.imread(test_image, cv2.IMREAD_GRAYSCALE)\n","#test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)  ##turn image gray\n","#['class 0(Dirty)', 'class 1(Clean)']\n","cv2_imshow(test_image)\n","test_image = np.expand_dims(test_image, axis=0)  #keras expects extra batch dimension, so doing this line turns (150,150,3) into (None, 150, 150, 3), which is what the input layer of the model expects"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":154,"status":"ok","timestamp":1647952331927,"user":{"displayName":"Travis Howard","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04316066016894535489"},"user_tz":420},"id":"x9UPTocKCJzY","outputId":"50be3925-6dea-490c-aaa1-65852c2a84a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.00308549 0.95865476]]\n"]}],"source":["#testing on single image\n","test_prediction = model.predict(x=test_image,batch_size=10,verbose=0)\n","print(test_prediction)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lf2RSonHzqab"},"outputs":[],"source":["#model.compile()\n","#note: fixed error by looking at first layer shape and using that in ImageDataGenerator\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Nz4zQAAmWwv"},"outputs":[],"source":["test_path = \"/content/drive/MyDrive/cse485/data/test02\"\n","#test_path = \"/content/drive/MyDrive/Magic Zoom & Foldscope Images/E. coli/Before Ecofiltro/Optical Microscope (100X)\"\n","#test_dir = \"/content/drive/MyDrive/Magic Zoom & Foldscope Images/E. coli/Before Ecofiltro\"\n","#test_class = \"Optical Microscope (100X)\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":158,"status":"ok","timestamp":1647928763962,"user":{"displayName":"Travis Howard","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04316066016894535489"},"user_tz":420},"id":"rlQTLqvgn9Y7","outputId":"997a9a92-b822-4de9-9712-8a2733810171"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 133 images belonging to 2 classes.\n"]}],"source":["#NOTE: ImageDataGenerator requires the classes to be in their own subfolders from the parent directory or else it won't work. See https://kylewbanks.com/blog/loading-unlabeled-images-with-imagedatagenerator-flowfromdirectory-keras\n","test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory = test_path, target_size=(150,150), classes=[\"clean\",\"dirty\"], batch_size=5, shuffle= False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ysJ9MVTomYvI"},"outputs":[],"source":["#predictions = model.predict(x = null, verbose=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X_qAMCZ9vfc2"},"outputs":[],"source":["test_imgs, test_labels = next(test_batches)\n","plotImages(test_imgs)\n","print(test_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BjaRFWnIwEX-"},"outputs":[],"source":["test_batches.classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RuIHVKlqwJRA"},"outputs":[],"source":["predictions = model.predict(x=test_batches,verbose=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"joHmA1b3wZ1p"},"outputs":[],"source":["np.round(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ljlQxcjP053o"},"outputs":[],"source":["cm = confusion_matrix(y_true=test_batches.classes, y_pred=np.argmax(predictions,axis=-1))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1647928817942,"user":{"displayName":"Travis Howard","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04316066016894535489"},"user_tz":420},"id":"LesYybYv1UMZ","outputId":"b3a78bcf-8e20-45d9-94f8-f4eb1c267505"},"outputs":[{"data":{"text/plain":["{'clean': 0, 'dirty': 1}"]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["test_batches.class_indices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q7mnRZLy1Yx1"},"outputs":[],"source":["cm_plot_labels = ['clean','dirty']\n","plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')"]},{"cell_type":"markdown","metadata":{"id":"d2br15nV2L1L"},"source":["# Import video and track blobs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XAafltLR2Q91"},"outputs":[],"source":["vidcap = cv2.VideoCapture('/content/drive/MyDrive/cse485/data/test_videos/good00.mp4')\n","success,image = vidcap.read()\n","count = 0\n","\n","while success:\n","  cv2.imwrite(\"frame%d.jpg\" % count, image)     # save frame as JPEG file      \n","  success,image = vidcap.read()\n","  print('Read a new frame: ', success)\n","  if success == True and count % 5 == 0:\n","    cv2_imshow(image)\n","  count += 1\n"]},{"cell_type":"markdown","metadata":{"id":"gXP_48j7t9DZ"},"source":["# Video Method 01"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JZ2dcRChuB9x"},"outputs":[],"source":["#Generate images from video and save them to directory\n","#generated_images_dir = \"/content/drive/MyDrive/cse485/data/test03/\"  #store frames from videos\n","generated_images_dir = \"/content/drive/MyDrive/cse485/data/test03_v2/\"  #store frames from videos\n","output_dir_imgs = \"/content/drive/MyDrive/cse485/data/test_videos_output/images/\" #output analysis images\n","output_dir_vid = \"/content/drive/MyDrive/cse485/data/test_videos_output/video/\" #output analysis video"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_0v4We7DuN0t"},"outputs":[],"source":["#PUT IMAGES FROM VIDEO INTO DIRECTORY\n","\n","#video = cv2.VideoCapture('/content/drive/MyDrive/cse485/data/test_videos/good00.mp4')\n","video = cv2.VideoCapture('/content/drive/MyDrive/cse485/data/test_videos/real_good.mp4')\n","success,image = video.read()\n","count = 0\n","\n","while success:\n","  dir = generated_images_dir + \"frame%d.jpg\" #path to save image to\n","  cv2.imwrite( dir % count, image)     # save frame      \n","  success,image = video.read()\n","  #print('Read a new frame: ', success)\n","\n","  if success == True and count % 5 == 0: #if successfully gotten frame, save image to directory\n","    print('Read a new frame: ', success)\n","    #cv2_imshow(image)\n","    #shutil.move(image,'/content/drive/MyDrive/cse485/data/test03')     \n","  count += 1"]},{"cell_type":"code","source":["# SET UP BLOB DETECTOR\n","params = cv2.SimpleBlobDetector_Params() \n","\n","params.filterByInertia = False\n","params.filterByColor = False\n","#params.minThreshold = 10;\n","#params.minThreshold = 10;\n","  \n","# Set Area filtering parameters \n","params.filterByArea = True\n","params.minArea = 5 #prev 10\n","params.maxArea = 55 #25\n","\n","\n","  \n","# Set Circularity filtering parameters \n","params.filterByCircularity = True\n","params.minCircularity = 0.1\n","  \n","# Set Convexity filtering parameters \n","params.filterByConvexity = True\n","params.minConvexity = 0.2 #0.2\n","      \n","\n","#create blob detector\n","detector = cv2.SimpleBlobDetector_create(params)"],"metadata":{"id":"SPlMvHHdf_A9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##TESTING IMAGE CROPS\n","img_name = \"/content/drive/MyDrive/cse485/data/test03_v2/frame0.jpg\"\n","og_img = cv2.imread(img_name, 1)                                                 #     img = img[center[0]-500:center[0]+500,center[1]-500:center[1]+500, :]\n","og_img = cv2.resize(og_img,(750,750))\n","keypoints = detector.detect(og_img)\n","\n","print(\"Keypoints detected:\" + str(len(keypoints)))\n","\n","x,y = keypoints[0].pt\n","x, y = int(x), int(y)\n","size = int(keypoints[0].size)\n","sizeHalf = int(size/2)\n","\n","print(\"size of keypoint 0: \" + str(size))\n","print(\"X: \" + str(x) + \", Y:\" + str(y))\n","print(\"Image with keypoint 0\")\n","im_with_keypoints = cv2.circle(og_img.copy(), (int(x),int(y)) , size, (0,243,255), 1 )\n","cv2_imshow(im_with_keypoints)\n","\n","\n","#print(\"Image with keypoint 0\")\n","#cv2_imshow(og_img)\n","cropped = og_img[y - size:y + size, x - size:x + size]  #[y1:y2,x1:x2]\n","\n","cropped = cv2.resize(cropped, (100,100))\n","cv2_imshow(cropped)"],"metadata":{"id":"QH9E4JHAgPpt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWgFAP_I1nKi"},"outputs":[],"source":["#METHOD 01: Get keypoints every frame using blob detector\n","\n","from keras.backend import maximum\n","import os\n","import math\n","from PIL.Image import NONE\n","\n","import xlwt\n","from xlwt import Workbook\n","\n","\n","\n","#the frames per second of the video\n","fps = 30 \n","\n","frame_index = 0\n","color = 0\n","\n","#total number of images in directory\n","total_image_count =  len(os.listdir(generated_images_dir))\n","max = total_image_count\n","\n","#get path to image directory\n","image_directory = [os.path.join(generated_images_dir,f) for f in os.listdir(generated_images_dir)]\n","\n","#whether to draw the keypoint indexs on the image\n","draw_kp_index = False\n","\n","previous_keypoints = []\n","\n","track_id = 0\n","tracking_objects = {} #store all tracked blobs\n","min_dist = 15 #minimum distance of keypoint to consider a blob as same blob\n","\n","# Create spreadsheet to store output data\n","wb = Workbook()\n","outputSheet = wb.add_sheet('Sheet 1', cell_overwrite_ok=True)\n","row = 0\n","outputSheet.write(row, 0, 'Blob ID')\n","outputSheet.write(row, 1, 'Start Position (X,Y)')\n","outputSheet.write(row, 2, 'End Position (X,Y)')\n","outputSheet.write(row, 3, 'Occurences')\n","outputSheet.write(row, 4, 'Average Diameter')\n","row += 1\n","\n","for img_name in image_directory:\n","\n","  if frame_index < max:\n","                                                                                          \n","    #READ IN IMAGE AND RESIZE TO 750x750                                                  \n","    img = cv2.imread(img_name, 1)                                                 \n","    img = cv2.resize(img,(750,750))\n","\n","    #GET CENTER OF IMAGE\n","    img_c = extract_image_center(img.copy(),  hcrop=40/2, wcrop=40/2) \n"," \n","    #set image that we will draw on\n","    im_with_keypoints = img_c\n","    \n","    #get image keypoints using blob detector defined from above\n","    keypoints = detector.detect(img_c)\n","\n","    if len(previous_keypoints) > 0:\n","\n","      if len(tracking_objects) == 0: ##if no tracked objects currently\n","\n","        keypoints_copy = keypoints.copy() # we cant update when looping, so we need a copy\n","\n","        for kp in keypoints_copy:\n","            for pkp in previous_keypoints:\n","\n","              x1, y1 = kp.pt\n","              x2,y2 = pkp.pt\n","              distance = math.hypot(x2 - x1, y2 - y1)\n","\n","              #WHEN DISTANCE IS < MINIMUM, WE CONSIDER THAT AS THE SAME BLOB, SO WE REMOVE THAT KEYPOINT AND UPDATE THE PREVIOUS KEYPOINT POSITION\n","              if distance < min_dist:          \n","                keypoints.remove(kp)\n","                continue\n","\n","        for kp in keypoints: ##add any remaining keypoints to tracked objects\n","           \n","              tracking_objects[track_id] = (kp.pt[0], kp.pt[1], kp.size, 1, kp.pt[0], kp.pt[1], kp.size, kp.size)  # ADD TRACKED ELEMENT ( X, Y , KEYPOINT RADIUS, OCCURENCE COUNT, STARTX, STARTY , SUM RADIUS, AVERAGE DIAMETER)\n","              #print(\"Added ID: %d , Distance between points (%d,%d) and (%d,%d) is: %d\" % (track_id, x1,y1,x2,y2 ,distance) )\n","              track_id += 1\n","\n","      elif len(tracking_objects) > 0:\n","\n","        tracking_objects_copy = tracking_objects.copy()\n","        keypoints_copy = keypoints.copy()\n","\n","        for object_id, tracked_kp_point in tracking_objects_copy.items():\n","\n","            object_exists = False #check if saved tracked objects exits. If the distance is small, we can assume that the object exists still\n","\n","            for kp in keypoints_copy: #kp refers to the NEWLY detected keypoints for the frame\n","\n","              x1, y1 = kp.pt #TODO might need to do pt[0] and pt[1] &&&&&&&&&&&&&&&&&&&&&&&\n","              x2,y2 = tracked_kp_point[0], tracked_kp_point[1]\n","              distance = math.hypot(x2 - x1, y2 - y1)\n","\n","              #WHEN DISTANCE IS < MINIMUM, WE CONSIDER THAT AS THE SAME BLOB, SO WE REMOVE THAT KEYPOINT AND UPDATE THE PREVIOUS KEYPOINT POSITION\n","              if distance < min_dist:   \n","\n","                object_exists = True\n","                tracking_objects[object_id] = (kp.pt[0], kp.pt[1], kp.size, tracked_kp_point[3] + 1, tracked_kp_point[4], tracked_kp_point[5], tracked_kp_point[6] + kp.size, (tracked_kp_point[6] + kp.size) / (tracked_kp_point[3] + 1)  ) ## Update old point with new point (update position and increment occurence count)\n","\n","                if kp in keypoints: #remove keypoint with small distanc\n","                  keypoints.remove(kp)\n","\n","                continue\n","            \n","            #REMOVE KEYPOINTS THAT WERE LOST/NOT FOUND DURING THE CURRENT FRAME\n","            if not object_exists:\n","              addSheetRow(outputSheet,row,object_id,tracked_kp_point[4],tracked_kp_point[5],tracked_kp_point[0],tracked_kp_point[1],tracked_kp_point[3], tracked_kp_point[7]) #add row to sheet\n","              row += 1\n","              tracking_objects.pop(object_id)\n","\n","\n","        for kp in keypoints: ##add any remaining keypoints to tracked objects\n","\n","          tracking_objects[track_id] = (kp.pt[0], kp.pt[1], kp.size, 1, kp.pt[0], kp.pt[1], kp.size, kp.size) # ADD TRACKED ELEMENT ( X, Y, KEYPOINT RADIUS, OCCURENCE COUNT, STARTX, STARTY , SUM RADIUS, AVERAGE DIAMETER)\n","          #print(\"Added ID: %d , Distance between points (%d,%d) and (%d,%d) is: %d\" % (track_id, x1,y1,x2,y2 ,distance) )\n","          track_id += 1\n","      \n","        #REMOVE DUPLICATE KEYPOINTS (SAME X,Y COORDS)\n","        tracking_objects_copy = tracking_objects.copy()\n","        for object_id, tracked_kp_point in tracking_objects_copy.items():\n","          for object_id2, tracked_kp_point2 in tracking_objects_copy.items():\n","              x1, y1 = int(tracked_kp_point[0]), int(tracked_kp_point[1])\n","              x2,y2 = int(tracked_kp_point2[0]), int(tracked_kp_point2[1])\n","\n","              if x1 == x2 and y1 == y2 and object_id != object_id2:\n","\n","                if tracked_kp_point[3] >= tracked_kp_point2[3]: #remove duplicate that has least occurences\n","                  if object_id2 in tracking_objects.keys():\n","                    addSheetRow(outputSheet,row,object_id2,tracked_kp_point2[4],tracked_kp_point2[5],tracked_kp_point2[0],tracked_kp_point2[1],tracked_kp_point2[3], tracked_kp_point2[7]) #add row to sheet\n","                    row += 1\n","                    tracking_objects.pop(object_id2)\n","                else:\n","                  if object_id in tracking_objects.keys():\n","                    addSheetRow(outputSheet,row,object_id,tracked_kp_point[4],tracked_kp_point[5],tracked_kp_point[0],tracked_kp_point[1],tracked_kp_point[3], tracked_kp_point[7]) #add row to sheet\n","                    row += 1\n","                    tracking_objects.pop(object_id) \n","                \n","      \n","\n","\n","        print(\"Tracked Blobs\")\n","        print(tracking_objects)\n","\n","    if frame_index >= max - 1: #WHEN WE ARE AT LAST FRAME ADD REMAINING BLOBS TO SPREADSHEET\n","      print(\"ended\")\n","      for object_id, tracked_kp_point in tracking_objects.items():\n","        addSheetRow(outputSheet,row,object_id,tracked_kp_point[4],tracked_kp_point[5],tracked_kp_point[0],tracked_kp_point[1],tracked_kp_point[3], tracked_kp_point[7]) #add row to sheet\n","        row += 1\n","\n","    #Draw the circle and text around keypoints\n","    for object_id, point in tracking_objects.items():\n","      x,y = point[0], point[1]\n","      radius = point[2] #size of keypoint area\n","      im_with_keypoints = cv2.circle(im_with_keypoints, (int(x),int(y)) , int(radius), (0,243,255), 1 )\n","      im_with_keypoints = cv2.putText(im_with_keypoints, str(object_id),(int(x),int(y - 7)), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,0,255), 1 )\n","\n","    \n","    #SHOW IMAGE\n","    cv2_imshow( im_with_keypoints)\n","\n","    #make copy of points\n","    previous_keypoints = keypoints.copy()\n","\n","    frame_index += 1\n","\n","    cv2.waitKey(1000)\n","    cv2.destroyAllWindows()\n","\n","    #SAVE OUTPUT ANALYSIS IMAGES\n","      #dir = output_dir_imgs + \"frame%d.png\" \n","      #cv2.imwrite( dir % frame_index, im_with_keypoints) \n","\n","    print(\"\\n========================\\n\")\n","\n","#CREATE OUTPUT SPREADHSEET\n","wb.save('/content/drive/MyDrive/cse485/data/output.xls')\n","\n","#CREATE ANALYSIS VIDEO\n","  #generateAnalysisVideo(output_dir_imgs, output_dir_vid, total_image_count) \n","\n","\n","\n","              #keypoints[0]\n","       # if cv2.KeyPoint_overlap(previous_keypoints[0] , keypoints[0])== 1:\n","         # print(\"OVERLAP\")\n","           #im_with_keypoints = cv2.putText(im_with_keypoints, \"kp_%d (%d,%d)\" % (kp_index, int(x), int(y)),(int(x),int(y) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,234,255), 1 )\n","           # #The flag DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS ensures the size of the circle corresponds to the size of blob. See https://docs.opencv.org/3.4/de/d30/structcv_1_1DrawMatchesFlags.html\n","            #cv2.drawKeypoints(img_c, keypoints, im_with_keypoints, (0,0,color), cv2.DRAW_MATCHES_FLAGS_DRAW_OVER_OUTIMG,)  #draws over previous image\n","          #im_with_keypoints = cv2.drawKeypoints(img_c, keypoints, np.array([]), (0,0,color), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS) # original\n","           # im_with_keypoints = cv2.drawKeypoints(im_with_keypoints, all_kp, np.array([]), (0,243,255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS) # original\n"]},{"cell_type":"code","source":["def addSheetRow(sheet, row, ID, startX, startY, endX, endY, occurences):\n","  sheet.write(row,0, str(ID))\n","  sheet.write(row,1, '(%d,%d)' % ( startX , startY  ) )\n","  sheet.write(row,2, '(%d,%d)' % ( endX , endY  ) )\n","  sheet.write(row,3, '%d' % occurences )\n"],"metadata":{"id":"0lCwRKS_jLVb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Writing to an excel \n","# sheet using Python\n","import xlwt\n","from xlwt import Workbook\n","  \n","# Workbook is created\n","wb = Workbook()\n","\n","# add_sheet is used to create sheet.\n","outputSheet = wb.add_sheet('Sheet 1', cell_overwrite_ok=True)\n","row = 0\n","\n","#(row, column)\n","outputSheet.write(row, 0, 'Blob ID')\n","outputSheet.write(row, 1, 'Start Position (X,Y)')\n","outputSheet.write(row, 2, 'End Position (X,Y)')\n","outputSheet.write(row, 3, 'Occurences')\n","row += 1\n","\n","addSheetRow(outputSheet,row,1,25,25,50,50,100)\n","row += 1\n","addSheetRow(outputSheet,row,2,27,25,80,50,691)\n","\n","\n","  \n","wb.save('/content/drive/MyDrive/cse485/data/output.xls')"],"metadata":{"id":"joHBU0T9iFpU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Je0SN41iwM2v"},"source":["# Testing Optical Flow tracking"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CasOIanVwUJZ"},"outputs":[],"source":["#set up variables\n","lk_params = dict(winSize  = (15, 15),\n","                maxLevel = 2,\n","                criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n","\n","feature_params = dict(maxCorners = 20,\n","                    qualityLevel = 0.7, #0.3 quality of tracking\n","                    minDistance = 1, #10\n","                    blockSize = 7 )\n","\n","\n","trajectory_len = 80\n","detect_interval = 1\n","trajectories = []\n","frame_idx = 0\n","\n","# SET UP BLOB DETECTOR\n","params = cv2.SimpleBlobDetector_Params() \n","  \n","# Set Area filtering parameters \n","params.filterByArea = False\n","#params.minArea = 0.1 #prev 10\n","#params.maxArea = 10\n","\n","params.filterByColor = False\n","  \n","# Set Circularity filtering parameters \n","params.filterByCircularity = False\n","#params.minCircularity = 0.9\n","  \n","# Set Convexity filtering parameters \n","params.filterByConvexity = False\n","#params.minConvexity = 0.2 #0.2\n","      \n","# Set inertia filtering parameters \n","params.filterByInertia = False\n","#params.minInertiaRatio = 0.01\n","\n","#create blob detector\n","detector = cv2.SimpleBlobDetector_create(params)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MaPzKGe2wttU"},"outputs":[],"source":["from keras.backend import maximum\n","import os\n","from PIL.Image import NONE\n","\n","\n","#total number of images in directory\n","total_image_count =  len(os.listdir(generated_images_dir))\n","max = total_image_count\n","\n","#get path to image directory\n","image_directory = [os.path.join(generated_images_dir,f) for f in os.listdir(generated_images_dir)]\n","#the frames per second of the video\n","fps = 30 \n","\n","frame_index = 0\n","color = 0\n","draw_path_trail = False #whether to show trail of movement\n","draw_bounding_box = False\n","draw_keypoints = False #whether to show the keypoints from the Blob detector\n","\n","previous_keypoints = None\n","overlapped_keypoints = []\n","\n","print(\"Getting images from:\" + generated_images_dir + \"\\nTotal images: \" + str(total_image_count) )\n","\n","for img_name in image_directory:\n","\n","  if frame_index < max:#total_image_count:\n","\n","    #read in image and resize to 750x750                                                 \n","    img = cv2.imread(img_name, 1)                                                \n","    img = cv2.resize(img,(750,750))\n","    img_c = extract_image_center(img.copy(),  hcrop=40/2, wcrop=40/2)  #get center of image\n","\n","    frame_gray = cv2.cvtColor(img_c, cv2.COLOR_BGR2GRAY) #optical flow image requires gray image\n","    img = img_c.copy()\n","\n","#-------------Keypoints\n","    #set image that we will draw on\n","   # im_with_keypoints = img_c\n","\n","    #get image keypoints using blob detector \n","    #keypoints = detector.detect(img_c)\n","\n","    #draw keypoints over image\n","   # im_with_keypoints = cv2.drawKeypoints(img_c, keypoints, np.array([]), (0,0,color), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS) \n","#-------------------------------\n","\n","    # Calculate optical flow for a sparse feature set using the iterative Lucas-Kanade Method\n","    if len(trajectories) > 0: #if any trajectories that were tracked or \n","\n","        img0, img1 = prev_gray, frame_gray # get previous frame and current frame\n","        p0 = np.float32([trajectory[-1] for trajectory in trajectories]).reshape(-1, 1, 2) #create points\n","        p1, _st, _err = cv2.calcOpticalFlowPyrLK(img0, img1, p0, None, **lk_params)\n","        p0r, _st, _err = cv2.calcOpticalFlowPyrLK(img1, img0, p1, None, **lk_params)\n","\n","        diff = abs(p0-p0r).reshape(-1, 2).max(-1) #get differnce between points\n","\n","        good = diff < 1 #if diff is less than 1 the optical flow is good\n","\n","        new_trajectories = []\n","\n","        # Get all the trajectories\n","        for trajectory, (x, y), good_flag in zip(trajectories, p1.reshape(-1, 2), good):\n","           \n","            if not good_flag:\n","                continue\n","\n","            trajectory.append((x, y)) #keep updating trajectory as we detect new points\n","\n","            if len(trajectory) > trajectory_len: #limit number of points displayed\n","                del trajectory[0]\n","\n","            new_trajectories.append(trajectory) #add new trajectory\n","\n","            # Newest detected point\n","            cv2.circle(img, (int(x), int(y)), 2, (0, 0, 255), -1) #creates red point that shows the closest point between two frames\n","            #cv2.putText(img, \"point_%d (%d,%d)\" % (1, int(x), int(y)),(int(x),int(y) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,234,255), 1 ) \n","            #print(\"point_%d (%d,%d)\" % (1, int(x), int(y)))\n","\n","          \n","        trajectories = new_trajectories #update our trajectories list\n","\n","        # Draw all the trajectories\n","        cv2.polylines(img, [np.int32(trajectory) for trajectory in trajectories], False, (0, 255, 0)) #creates the green lines that follows the red point, which shows trajectories\n"," \n","        #cv2.putText(img, 'track count: %d' % len(trajectories), (20, 50), cv2.FONT_HERSHEY_PLAIN, 1, (0,255,0), 2)\n","\n","\n","    # Update interval - When to update and detect new features\n","    if frame_idx % detect_interval == 0:\n","\n","        #Create two masks. mask1 is used for detecting good features in optimal flow, mask2 is used to generate bounding box around blob contours\n","        mask = np.zeros_like(frame_gray) #Return an array of zeros with the same shape and type as a given array\n","        mask[:] = 255 #turn image white\n","        mask2 = np.zeros_like(frame_gray) #Return an array of zeros with the same shape and type as a given array\n","        mask2[:] = 0 #turn image black\n","\n","        t = 0\n","        z=[(0,0)]\n","        # Lastest point in latest trajectory\n","        for x, y in [np.int32(trajectory[-1]) for trajectory in trajectories]:\n","\n","          if not (x,y) in z:\n","            #print(\"Added point at \" + str(x) + \",\" + str(y))\n","            cv2.circle(mask2, (x, y), 5, 255, -1) #draw white circles on MASK image\n","            #cv2.putText(img, \"point_%d (%d,%d)\" % (1, int(x), int(y)),(int(x),int(y) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,234,255), 1 )\n","            z.append((x,y))\n","            z.append((x + 1, y - 1)) #trim out points that are too close\n","            z.append((x - 1, y + 1)) #trim out points that are too close\n","            z.append((x + 1, y)) #trim out points that are too close\n","            z.append((x - 1, y)) #trim out points that are too close\n","            z.append((x , y + 1)) #trim out points that are too close\n","            z.append((x , y - 1)) #trim out points that are too close\n","            z.append((x + 1,y + 1)) #trim out points that are too close\n","            z.append((x - 1,y - 1))\n","            t+=1\n","\n","          #cv2.circle(mask2, (x, y), 5, 255, -1) #draw white circles on MASK image\n","         # z.append((x,y))\n","        \n","          #print(\"Blobs Detected: \" + str(t))\n","          #Get countours for mask image, then draw bounding rectangle around countours\n","          contours, _ = cv2.findContours(mask2,cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","          if contours:\n","            for cnt in contours:\n","              x1,y1,w1,h1 = cv2.boundingRect(cnt)\n","              cv2.rectangle(img, (x1,y1), (x1+w1, y1+h1), (0,0,255), 1) #draw bound box around movement area\n","      \n","              \n","\n","        # Detect the good features to track\n","        p = cv2.goodFeaturesToTrack(frame_gray, mask = mask, **feature_params)\n","        if p is not None:\n","            # If good features can be tracked - add that to the trajectories\n","            for x, y in np.float32(p).reshape(-1, 2):\n","                trajectories.append([(x, y)])\n","\n","\n","    frame_idx += 1\n","    prev_gray = frame_gray\n","\n","    if frame_idx > 0:#max - 12:\n","      #show analysis image\n","      if frame_idx > 149:\n","        #cv2_imshow(img) \n","        print(\"\")\n","    #save output analysis images\n","    dir = output_dir_imgs + \"frame%d.png\" \n","    cv2.imwrite( dir % frame_idx, img)   \n","\n","\n","\n","    #cv2_imshow(mask2)\n","    cv2.waitKey(100)\n","    cv2.destroyAllWindows()\n","    frame_index += 1\n","\n","#generateAnalysisVideo(output_dir_imgs, output_dir_vid, total_image_count) #create video from the analysis images"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"CNN_test.ipynb","provenance":[],"authorship_tag":"ABX9TyOJ1Ngl8cRemB49+Zh/9btT"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}